use this simple concept in a chain. We first showcase ConversationBufferMemory which is just a wrapper around ChatMessageHistory that extracts the messages in a variable.We can first extract it as a string.from langchain.memory import ConversationBufferMemorymemory = ConversationBufferMemory()memory.chat_memory.add_user_message("hi!")memory.chat_memory.add_ai_message("whats up?")memory.load_memory_variables({})    {'history': 'Human: hi!\nAI: whats up?'}We can also get the history as a list of messagesmemory = ConversationBufferMemory(return_messages=True)memory.chat_memory.add_user_message("hi!")memory.chat_memory.add_ai_message("whats up?")memory.load_memory_variables({})    {'history': [HumanMessage(content='hi!', additional_kwargs={}),      AIMessage(content='whats up?', additional_kwargs={})]}Using in a chainâ€‹Finally, let's take a look at using this in a chain (setting verbose=True so we can see the prompt).from langchain.llms import OpenAIfrom langchain.chains import ConversationChainllm = OpenAI(temperature=0)conversation = ConversationChain(    llm=llm,    verbose=True,    memory=ConversationBufferMemory())conversation.predict(input="Hi there!")    > Entering new ConversationChain chain...    Prompt after formatting:    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.    Current conversation:    Human: Hi there!    AI:    > Finished chain.    " Hi there! It's nice to meet you. How can I help you today?"conversation.predict(input="I'm doing well! Just having a conversation