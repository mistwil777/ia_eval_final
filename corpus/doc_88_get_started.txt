split the documents into chunks.from langchain.text_splitter import CharacterTextSplittertext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_documents(documents)We will then select which embeddings we want to use.from langchain.embeddings import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()We now create the vectorstore to use as the index.from langchain.vectorstores import Chromadb = Chroma.from_documents(texts, embeddings)    Running Chroma using direct local API.    Using DuckDB in-memory for database. Data will be transient.So that's creating the index. Then, we expose this index in a retriever interface.retriever = db.as_retriever()Then, as before, we create a chain and use it to answer questions!qa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type="stuff", retriever=retriever)query = "What did the president say about Ketanji Brown Jackson"qa.run(query)    " The President said that Judge Ketanji Brown Jackson is one of the nation's top legal minds, a former top litigator in private practice, a former federal public defender, and from a family of public school educators and police officers. He said she is a consensus builder and has received a broad range of support from organizations such as the Fraternal Order of Police and former judges appointed by Democrats and Republicans."VectorstoreIndexCreator is just a wrapper around all this logic. It is configurable in the text splitter it uses, the embeddings it uses, and the vectorstore it uses. For example, you can configure it as below:index_creator = VectorstoreIndexCreator(    vectorstore_cls=Chroma,    embedding=OpenAIEmbeddings(),    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0))Hopefully this highlights