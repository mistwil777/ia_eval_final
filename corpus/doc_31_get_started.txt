The types of messages currently supported in LangChain are AIMessage, HumanMessage, SystemMessage, and ChatMessage -- ChatMessage takes in an arbitrary role parameter. Most of the time, you'll just be dealing with HumanMessage, AIMessage, and SystemMessage__call__​Messages in -> message out​You can get chat completions by passing one or more messages to the chat model. The response will be a message.from langchain.schema import (    AIMessage,    HumanMessage,    SystemMessage)chat([HumanMessage(content="Translate this sentence from English to French: I love programming.")])    AIMessage(content="J'aime programmer.", additional_kwargs={})OpenAI's chat model supports multiple messages as input. See here for more information. Here is an example of sending a system and user message to the chat model:messages = [    SystemMessage(content="You are a helpful assistant that translates English to French."),    HumanMessage(content="I love programming.")]chat(messages)    AIMessage(content="J'aime programmer.", additional_kwargs={})generate​Batch calls, richer outputs​You can go one step further and generate completions for multiple sets of messages using generate. This returns an LLMResult with an additional message parameter.batch_messages = [    [        SystemMessage(content="You are a helpful assistant that translates English to French."),        HumanMessage(content="I love programming.")    ],    [        SystemMessage(content="You are a helpful assistant that translates English to French."),        HumanMessage(content="I love artificial intelligence.")    ],]result = chat.generate(batch_messages)result    LLMResult(generations=[[ChatGeneration(text="J'aime programmer.", generation_info=None,