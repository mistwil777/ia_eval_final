Skip to main content🦜️🔗 LangChainJS/TS DocsGitHubCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/​OPromptsLanguage modelsLLMsChat modelsHow-toIntegrationsOutput parsersData connectionChainsMemoryAgentsCallbacksModulesUse casesGuidesEcosystemAdditional resourcesAPI referenceModulesModel I/​OLanguage modelsChat modelsOn this pageChat modelsChat models are a variation on language models.
While chat models use language models under the hood, the interface they expose is a bit different.
Rather than expose a "text in, text out" API, they expose an interface where "chat messages" are the inputs and outputs.Chat model APIs are fairly new, so we are still figuring out the correct abstractions.The following sections of documentation are provided:How-to guides: Walkthroughs of core functionality, like streaming, creating chat prompts, etc.Integrations: How to use different chat model providers (OpenAI, Anthropic, etc).Get started​Setup​To start we'll need to install the OpenAI Python package:pip install openaiAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:export OPENAI_API_KEY="..."If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:from langchain.chat_models import ChatOpenAIchat = ChatOpenAI(open_api_key="...")otherwise you can initialize without any params:from langchain.chat_models import ChatOpenAIchat = ChatOpenAI()Messages​The chat model interface is based around messages rather than raw text.