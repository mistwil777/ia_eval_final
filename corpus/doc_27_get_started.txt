other LLM provider-specific information:llm_result = llm.generate(["Tell me a joke", "Tell me a poem"]*15)len(llm_result.generations)    30llm_result.generations[0]    [Generation(text='\n\nWhy did the chicken cross the road?\n\nTo get to the other side!'),     Generation(text='\n\nWhy did the chicken cross the road?\n\nTo get to the other side.')]llm_result.generations[-1]    [Generation(text="\n\nWhat if love neverspeech\n\nWhat if love never ended\n\nWhat if love was only a feeling\n\nI'll never know this love\n\nIt's not a feeling\n\nBut it's what we have for each other\n\nWe just know that love is something strong\n\nAnd we can't help but be happy\n\nWe just feel what love is for us\n\nAnd we love each other with all our heart\n\nWe just don't know how\n\nHow it will go\n\nBut we know that love is something strong\n\nAnd we'll always have each other\n\nIn our lives."),     Generation(text='\n\nOnce upon a time\n\nThere was a love so pure and true\n\nIt lasted for centuries\n\nAnd never became stale or dry\n\nIt was moving and alive\n\nAnd the heart of the love-ick\n\nIs still beating strong and true.')]You can also access provider specific information that is returned. This information is NOT standardized across providers.llm_result.llm_output    {'token_usage': {'completion_tokens': 3903,      'total_tokens': 4023,      'prompt_tokens': 120}}Edit this pagePreviousLanguage modelsNextAsync APIGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright