FLAREThis notebook is an implementation of Forward-Looking Active REtrieval augmented generation (FLARE).📄️ Graph DB QA chainThis notebook shows how to use LLMs to provide a natural language interface to a graph database you can query with the Cypher query language.📄️ KuzuQAChainThis notebook shows how to use LLMs to provide a natural language interface to Kùzu database.📄️ NebulaGraphQAChainThis notebook shows how to use LLMs to provide a natural language interface to NebulaGraph database.📄️ Graph QAThis notebook goes over how to do question answering over a graph data structure.📄️ Hypothetical Document EmbeddingsThis notebook goes over how to use Hypothetical Document Embeddings (HyDE), as described in this paper.📄️ Bash chainThis notebook showcases using LLMs and a bash process to perform simple filesystem commands.📄️ Self-checking chainThis notebook showcases how to use LLMCheckerChain.📄️ Math chainThis notebook showcases using LLMs and Python REPLs to do complex word math problems.📄️ HTTP request chainUsing the request library to get HTML results from a URL and then an LLM to parse results📄️ Summarization checker chainThis notebook shows some examples of LLMSummarizationCheckerChain in use with different types of texts.  It has a few distinct differences from the LLMCheckerChain, in that it doesn't have any assumptions to the format of the input text (or summary).📄️ ModerationThis notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like