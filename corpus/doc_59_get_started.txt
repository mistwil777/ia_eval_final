Skip to main content🦜️🔗 LangChainJS/TS DocsGitHubCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/​OData connectionChainsMemoryAgentsCallbacksHow-toIntegrationsModulesUse casesGuidesEcosystemAdditional resourcesAPI referenceModulesCallbacksCallbacksLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.Callback handlers​CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered.class BaseCallbackHandler:    """Base callback handler that can be used to handle callbacks from langchain."""    def on_llm_start(        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any    ) -> Any:        """Run when LLM starts running."""    def on_chat_model_start(        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any    ) -> Any:        """Run when Chat Model starts running."""    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:        """Run on new LLM token. Only available when streaming is enabled."""    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:        """Run when LLM ends running."""